\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{amsmath}



\title{Evaluating Modern Video Object Detection Architectures for Industrial Environments}

\author{ Erik Ilyassov \\
% \thanks{Use footnote for providing further
% 		information about author (webpage, alternative
% 		address)---\emph{not} for acknowledging funding agencies.} \\
	Artificial Intelligence Center \\
	Skolkovo Institute of Science and Technology\\
	Moscow, Russia \\
	\texttt{E.Ilyassov@skoltech.ru} \\
	%% examples of more authors
	\And
	Svetlana Illarionova \\
	Artificial Intelligence Center \\
	Skolkovo Institute of Science and Technology\\
	Moscow, Russia \\
	\texttt{S.Illarionova@skoltech.ru} \\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}

\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
Modern deep learning techniques have enabled significant progress in video object detection (VOD). However, transferring these models to industrial environments remains challenging due to the scarcity of domain‑specific data, high annotation costs and large domain shifts. We conduct a systematic study of state‑of‑the‑art VOD architectures on a new industrial dataset containing objects such as wheels, pallets, containers and forklifts. To compensate for limited annotations we curate a supplementary assembly dataset and collect unlabelled videos from the target domain. The backbone is first adapted using self‑supervised video representation learning on the unlabelled corpus, then fine‑tuned on the labelled data with and without the assembly set. By varying the temporal window length and initialization, we show that the combination of ImageNet‑VID pretraining, assembly‑set adaptation and self‑supervised learning yields the best accuracy. In particular, a 10‑frame temporal window achieves the highest average precision when using the augmented dataset, outperforming both shorter (1–5 frames) and longer (15 frames) contexts. Ablation studies confirm the importance of query fusion and pretraining. Our results provide practical guidelines for deploying modern VOD models in challenging industrial settings.
\end{abstract}


% \keywords{Video object detection \and Video instance segmentation \and Transformers \and Industrial video analytics}

\section{Introduction}

Automated video analytics in industrial and operational environments is becoming an essential component of safety assurance, quality inspection, and process optimization. Unlike controlled or static scenes, industrial video streams are characterized by high motion variability, frequent occlusions, illumination changes, specular reflections, dust, and severe constraints on latency and computational budget. In such conditions, the system must not only detect objects and delineate their spatial boundaries, but also maintain instance consistency over time, even when visual appearance is unstable. The deployment of modern computer vision models in production remains limited due to the scarcity of domain-specific datasets, the high cost of annotation, and significant domain shifts arising from non-standard imaging conditions.

Recent advances in \emph{video object detection} (VOD) and \emph{video instance segmentation} (VIS) have demonstrated impressive progress in modeling spatio-temporal dependencies. One major direction focuses on \textbf{end-to-end transformer architectures}, which jointly encode temporal and spatial information using object-query representations. TransVOD and its improved variant TransVOD++ \citep{TransVOD2022,TransVODpp2023} formulate VOD as a set-prediction problem and aggregate object-level context across frames, thereby eliminating the need for external optical flow or handcrafted post-processing such as Seq-NMS. Follow-up works further refine these ideas by enforcing temporal coherence and identity consistency at the feature or query level \citep{ClipVID2023,ICA2023}. 

Another active research line pursues \textbf{efficient one-stage solutions} that achieve a favorable balance between accuracy and latency. Real-time DETR \citep{RTDETR2023} and the latest YOLO family (e.g., YOLOv10) \citep{YOLOv10_2024} exemplify this trend, providing strong performance under real-time constraints. Extensions of one-stage VOD models leverage temporal redundancy to skip redundant computation without degrading accuracy \citep{EfficientVOD2024}.  

In parallel, progress in \textbf{video instance segmentation (VIS)} has led to frameworks that couple detection and mask prediction at the sequence level. Methods such as MinVIS and SeqFormer \citep{MinVIS2022,SeqFormer2022} show that strong image detectors can serve as temporal models with minimal additional supervision, while VISAGE and SyncVIS \citep{VISAGE2023,SyncVIS2024} incorporate explicit spatio-temporal attention to improve tracking consistency and appearance modeling. Open-vocabulary formulations such as OV2Seg and OVFormer extend category coverage by aligning visual and language embeddings \citep{OV2Seg2023,OVFormer2024,LVVIS2024}.

Backbone choice plays a crucial role in domain generalization. Vision Transformers (ViT) \citep{ViT2020} and hierarchical Swin Transformers \citep{Swin2021} have become standard backbones for both DETR-style and one-stage detectors, offering high-quality transferable representations. Their self-supervised or domain-adapted variants further enhance robustness when training data is limited.

The evaluation of video detection and segmentation models typically relies on large-scale public benchmarks. ImageNet-VID \citep{ImageNetVID2015} and YouTube-VIS/OVIS \citep{YouTubeVIS2019,OVIS2022} are commonly used for measuring frame-level accuracy and occlusion robustness. Datasets such as UA-DETRAC \citep{UADETRAC2015} and BDD100K \citep{BDD100K2020} extend this to real-world driving and surveillance scenarios, while LV-VIS \citep{LVVIS2024} enables open-vocabulary assessment. However, these benchmarks poorly represent industrial environments, which feature unique object categories, visual artifacts, and lighting conditions, leading to substantial domain shifts in practice.

In this study, we present a systematic and reproducible comparison of contemporary VOD models with emphasis on their \emph{industrial applicability}. We focus on end-to-end transformer-based architectures (TransVOD/TransVOD++) and adapt them to our domain by supplementing the labelled dataset with a curated assembly dataset from public sources and by employing self-supervised learning on unlabelled industrial videos to adapt the Swin backbone. Experiments conducted on public benchmarks and our industrial dataset reveal that combining ImageNet‑VID pretraining with assembly data and SSL leads to the highest accuracy. A moderate temporal window of ten frames provides the best balance between accuracy and stability when auxiliary data are used, whereas shorter windows suffice in the absence of such data. These findings offer practical guidelines for deploying modern VOD models in industrial monitoring and highlight the importance of domain-specific pretraining and temporal context.






\section{Related Work}

\paragraph{Early feature aggregation for VOD.}
Classical video object detection (VOD) methods improve per-frame detectors by aggregating temporal evidence. T-CNN links detections into tubelets with temporal smoothness and rescoring \citep{TCNN2016}, while DFF propagates deep features with optical flow to save computation and stabilise predictions \citep{DFF2017}. FGFA follows with flow-guided feature fusion to better handle motion blur and small objects \citep{FGFA2017}. Memory- and relation-based designs advance this line: STMN introduces spatial–temporal memory for long-range cues \citep{STMN2018}; RDN distils relation reasoning from heavy teachers to lightweight students \citep{RDN2019}; SELSA aggregates sequence-level semantics \citep{SELSA2019}; MEGA unifies global–local memory with alignment to combat occlusions and appearance changes \citep{MEGA2020}. In parallel, track–detect hybrids such as Detect-to-Track and Track-to-Detect emphasise identity consistency and motion priors \citep{DT2017}.

\paragraph{End-to-end set prediction and spatio-temporal transformers.}
DETR reframes detection as bipartite matching with a fixed set of queries \citep{DETR2020}, and Deformable DETR improves convergence with multi-scale deformable attention \citep{DeformableDETR2021}. Building on this, TransVOD aggregates temporal context at the object-query level and removes reliance on optical flow or Seq-NMS \citep{TransVOD2022}, with TransVOD++ strengthening temporal fusion and redundancy reduction for higher robustness \citep{TransVODpp2023}. Identity-consistent aggregation and clip-wise variants further enhance temporal coherence while preserving end-to-end training \citep{ICA2023,ClipVID2023}. On the efficiency front, real-time DETR (RT-DETR) delivers competitive accuracy–latency trade-offs \citep{RTDETR2023}, and modern one-stage families (e.g., YOLOv10) continue to push real-time boundaries relevant for industrial constraints \citep{YOLOv10_2024}. These systems commonly rely on transformer backbones and hierarchical encoders such as ViT and Swin \citep{ViT2020,Swin2021}.

\paragraph{Video instance segmentation (VIS).}
VIS couples detection, association, and mask prediction. Early pipelines extend image instance segmentation with temporal links (MaskTrack R-CNN) \citep{MaskTrackRCNN2019}. Query-based transformers (VisTR) bring end-to-end set prediction to VIS \citep{VisTR2021}. Sequence-level models (SeqFormer) explicitly maintain object-level temporal queries \citep{SeqFormer2022}. Minimalist formulations (MinVIS) demonstrate the strength of image detectors with principled matching \citep{MinVIS2022}. Subsequent works improve temporal association and appearance modeling under occlusion and motion (e.g., VISAGE, SyncVIS) \citep{VISAGE2023,SyncVIS2024}, while large-scale designs (e.g., VITA) combine long-term aggregation with efficient memory to stabilize masks and identities \citep{VITA2023}. Generalised frameworks (e.g., GenVIS) highlight unified modeling choices and training protocols \citep{GenVIS2023}.

\paragraph{Open-vocabulary and described-query video understanding.}
To alleviate category coverage gaps, open-vocabulary VIS and VOD align visual features with language supervision. OV2Seg extends open-vocabulary ideas to video segmentation \citep{OV2Seg2023}; OVFormer brings transformer-based alignment and decoupled heads \citep{OVFormer2024}. Recent datasets and tasks explore natural-language or described-query conditions; for example, DSTVD formalises described spatio-temporal detection with strong baselines adapted from tube and set-prediction detectors \citep{DSTVD2025}. Open-vocabulary VIS/VOD is especially relevant to industrial deployments, where novel object types and frequent class drift are common.

\paragraph{Datasets and evaluation under distribution shift.}
ImageNet-VID remains a standard for VOD \citep{ImageNetVID2015}, while YouTube-VIS and OVIS stress-test occlusions and long-term consistency for VIS \citep{YouTubeVIS2019,OVIS2022}. Driving-centric and surveillance datasets (UA-DETRAC, BDD100K) probe robustness to illumination, weather, and scale changes \citep{UADETRAC2015,BDD100K2020}. TAO broadens category and domain diversity for generic video-level evaluation \citep{TAO2020}; LV-VIS expands vocabulary to assess open-vocabulary generalisation \citep{LVVIS2024}. For our industrial focus, these benchmarks provide complementary signals but still under-represent production artefacts (specularities, dust, periodic motion), motivating domain-specific data and data-selection strategies.

\paragraph{Summary.}
Overall, the field has progressed from flow-guided aggregation and memory-based fusion to end-to-end query transformers with stronger identity modeling. Efficient one-stage and real-time designs make these advances practical under latency budgets; VIS methods couple detection and masks with sequence-level reasoning; and open-vocabulary formulations start to address label scarcity. Our study leverages these developments to compare representative architectures under industrial constraints and to quantify how data selection and domain shift influence stability and accuracy.



\section{Problem Formulation}

\subsection{Data}
We consider a video $V=(I_t)_{t=1}^{T}$, where each frame $I_t:\Omega\!\to\!\mathbb{R}^3$ is an RGB image over pixel domain $\Omega\subset\mathbb{R}^2$. A training corpus $\mathcal{D}=\{(V^{(n)},Y^{(n)})\}_{n=1}^{N}$ is drawn i.i.d.\ from an unknown distribution $\mathbb{P}(V,Y)$ that reflects industrial environments (illumination shifts, motion blur, occlusions, periodic operations).

In addition to the labelled corpus $\mathcal{D}$ we make use of two auxiliary data sources.  The first is a \emph{supplementary assembly dataset} comprising thousands of images of mechanical parts, pallets, boxes and conveyors collected from public industrial footage.  This dataset is used for intermediate pretraining between ImageNet‑VID and our target domain.  The second is an unlabeled video corpus captured in the same industrial environment, which enables self‑supervised pretraining of the backbone via masked or contrastive learning.  In the experiments we form clips of length $T$ from each video and treat $T\in\{1,5,10,15\}$ as a hyperparameter.  Empirical results show that a context of ten frames yields the highest accuracy when the model leverages the assembly dataset and self‑supervised pretraining, while shorter contexts suffice in the absence of auxiliary data.
For \emph{video object detection (VOD)} the annotation at time $t$ is a finite set
\[
Y_t=\{(c_{t,k},\,\mathbf{b}_{t,k})\}_{k=1}^{K_t},\qquad
\mathbf{b}_{t,k}\in[0,1]^4,\;\; c_{t,k}\in\{1,\dots,C\},
\]
and $Y=(Y_t)_{t=1}^{T}$. Here $\mathbf{b}_{t,k}$ denotes a normalized bounding box (e.g., $(x_\text{min},y_\text{min},x_\text{max},y_\text{max})$ in relative coordinates). If instance masks are available, each tuple extends to $(c_{t,k},\mathbf{b}_{t,k},m_{t,k})$ with $m_{t,k}\in\{0,1\}^{\Omega}$. Temporal identities are not required for VOD but can be derived a posteriori. Algebraically, $(Y_t)$ is a \emph{sequence of finite sets}; probabilistically, $(V,Y)\sim\mathbb{P}$ with latent temporal dynamics and nuisance factors.

\subsection{Mapping $f:\mathcal{X}\to\mathcal{Y}$ (model-agnostic)}
Given a context clip $X=(I_{t-\ell},\dots,I_t,\dots,I_{t+r})\in\mathcal{X}$ centered at time $t$, the predictor returns a permutation-invariant set of detections for frame $t$:
\[
f_\theta(X)=\hat{Y}_t=\{(\hat c_j,\hat{\mathbf{b}}_j,\hat m_j)\}_{j=1}^{\hat K_t},
\]
where masks $\hat m_j$ are optional (VIS). We adopt a standard four-block factorization
\[
f_\theta \;=\; \underbrace{H}_{\text{head / output}}
\circ \underbrace{D}_{\text{object decoder}}
\circ \underbrace{A}_{\text{temporal aggregation}}
\circ \underbrace{\phi}_{\text{backbone}}.
\]
\textbf{Backbone $\phi$:} per-frame feature pyramid $F_s=\phi(I_s)$ using a 2D CNN (e.g., ResNet/Swin), ViT/Video-ViT (e.g., Swin-Video, TimeSformer), or hybrids.
\textbf{Temporal aggregation $A$:} builds context for $t$,
$Z_t = A(F_{t-\ell},\dots,F_{t+r})$, via (i) 3D convolutions/temporal shift, (ii) learned alignment and summation (flow-free or flow-based warp), (iii) recurrent/memory mechanisms, or (iv) spatio-temporal attention (Transformers).
\textbf{Object decoder $D$:} maps $Z_t$ to object slots $\{s_j\}$; in query-based designs (DETR-style) a fixed set of queries is trained with Hungarian matching; dense/anchor heads with NMS are an alternative.
\textbf{Head $H$:} predicts class $\hat c_j$, box $\hat{\mathbf{b}}_j$, and (if applicable) mask $\hat m_j=\psi(s_j,Z_t)$ with an upsampling/deformable-attention mask decoder. Post-processing is minimal for set-prediction (no NMS), optional for dense heads. Both causal (online, $r{=}0$) and offline ($r{>}0$) regimes are covered; open-vocabulary variants replace the classifier by a vision–text similarity head.

\paragraph{Instantiation: TransVOD++.}
A concrete instantiation fits the above template: $\phi$ is an image backbone plus spatial transformer encoder/decoder (Deformable-DETR style); $A$ is a \emph{Temporal Query Encoder} (TQE) that aggregates object queries across the clip; $D$ is a \emph{Temporal Deformable Decoder} (TDTD) that attends to temporal memories to produce current-frame predictions; $H$ is the detection head (optionally with masks). Internal modules such as \emph{Query-and-RoI Fusion} (QRF) and \emph{Hard Query Mining} (HQM) inject appearance cues and reduce redundancy while preserving the end-to-end set-prediction interface.

\subsection{External evaluation criterion}
The primary metric is mean Average Precision (mAP) over IoU thresholds on a held-out video set, computed per frame and averaged over classes (ImageNet-VID protocol). For deployment-oriented reporting we additionally measure throughput (FPS) and latency (ms) on a fixed hardware profile, capturing the speed–accuracy trade-off required in industrial monitoring.

\subsection{Learning objective}
Training follows empirical risk minimization with Hungarian set matching between predictions and ground truth. For each clip and time $t$, let $\pi$ be the optimal bipartite assignment between predicted slots and ground-truth objects. The per-frame detection loss is
\begin{align*}
\mathcal{L}_t(\theta)=\frac{1}{K_t}\sum_{k=1}^{K_t}\Big[
&\lambda_{\text{cls}}\;\mathcal{L}_{\text{cls}}\!\big(p_{\pi(k)}, c_{t,k}\big)
+ \lambda_{1}\;\|\hat{\mathbf{b}}_{\pi(k)}-\mathbf{b}_{t,k}\|_1 \\
&+ \lambda_{\text{giou}}\;\mathcal{L}_{\text{GIoU}}\!\big(\hat{\mathbf{b}}_{\pi(k)}, \mathbf{b}_{t,k}\big)
\Big],
\end{align*}
with focal or cross-entropy classification loss and GIoU-based localization. The total objective (with optional auxiliary decoder losses at intermediate layers, as in TransVOD++) is
\[
\min_{\theta}\;\frac{1}{N}\sum_{n=1}^{N}\frac{1}{T^{(n)}}\sum_{t=1}^{T^{(n)}}
\Big(\mathcal{L}_t^{(n)}(\theta)+\sum_{j\in\mathcal{J}}\alpha_j\,\mathcal{L}^{(n)}_{t,j}(\theta)\Big),
\]
optionally under a deployment constraint on average inference time
$\tau(f_\theta)\le \tau_{\max}$ (or via a Lagrangian penalty $\beta\,\tau(f_\theta)$)
to encode real-time requirements in industrial settings.



% ============================
% Proposed Solution
% ============================
\section{Proposed Solution}
\label{sec:method}

Our study investigates how to adapt an end‑to‑end transformer‑based video object detection framework to a demanding industrial setting. 
Unlike generic benchmarks, our internal dataset comprises warehouse and factory footage containing \texttt{wheel}, \texttt{pallet}, \texttt{container}, \texttt{forklift}, \texttt{box}, \texttt{barcode} and various sub‑components found on assembly lines, such as gears, conveyor belts and mechanical housings. 
These videos exhibit frequent motion blur, occlusions and variable artificial lighting, and the class distribution is long‑tailed. 
To enlarge the pool of relevant examples and mitigate annotation cost, we curate an additional assembly dataset from public industrial videos and images on the internet.  This \emph{assembly supplementary set} consists of thousands of images depicting mechanical parts, pallets, crates and packaging stations that overlap semantically with our internal categories. 
The supplementary set is used as an intermediate fine‑tuning stage between generic ImageNet‑VID pretraining and our labelled data. 
Furthermore, because the semantic gap between natural images and industrial scenes is substantial, we adapt the Swin‑B backbone via a self‑supervised video representation learning task on our unlabelled industrial recordings before any supervised fine‑tuning.  The self‑supervised stage relies on temporal consistency and masked modeling to teach the backbone the textures and dynamics of industrial environments.
Given these considerations, the following experiments systematically vary the initialization strategy, temporal context length and data composition to quantify their impact on accuracy and temporal stability.

\vspace{4pt}
\noindent\textbf{Training protocol.}
After pretraining the Swin‑B backbone using the self‑supervised task described above, we fine‑tune TransVOD++ in two stages. 
During a warm‑up phase only the detection heads and temporal transformer blocks are optimized while the backbone remains frozen. 
Subsequently, the last backbone stage is unfrozen and fine‑tuned with a $10\times$ smaller learning rate than the rest of the network to introduce modest domain adaptation without destabilising training. 
All experiments use synchronized frame augmentations, a cosine learning‑rate schedule and standard weight decay. 
Inference employs a sliding temporal window with stride one and simple box averaging for temporally matched detections. 
We report COCO‑style average precision (AP, AP$_{50}$, AP$_{75}$) and the mean consistency IoU (MC‑IoU), which measures the stability of detections across adjacent frames.

% =====================
% Experiments
% =====================
\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}

The internal dataset contains multi‑site industrial videos, divided into train/validation/test splits with no overlap between recording locations. 
In addition to this core collection we introduce the assembly supplementary set described in Section~\ref{sec:method}, which is used for intermediate pretraining. 
Unless otherwise stated, clips of $T{=}5$ consecutive frames are used for training and evaluation, and the Swin‑B backbone is first adapted on our unlabelled recordings using a self‑supervised task. 
Each frame undergoes identical photometric transformations to preserve temporal coherence. 
Evaluation is performed on the held‑out test split using the same metrics across all ablations.

% -------------------------------------------------
% Finetuning regimes and initialization
% -------------------------------------------------
\subsection{Fine-tuning regimes and initialization}

We first evaluate the effect of different initialization schemes. 
Four variants are considered: (i) training TransVOD++ entirely from scratch on our labelled industrial data; (ii) fine‑tuning from the authors’ ImageNet‑VID checkpoint; (iii) fine‑tuning from the ImageNet‑VID checkpoint after intermediate training on the assembly supplementary set; and (iv) fine‑tuning from a checkpoint that additionally employs our self‑supervised pretraining on unlabelled industrial videos (denoted as SSL). 
Table~\ref{tab:init} summarises the results.  Training from scratch performs poorly because our dataset is too small to learn stable spatio‑temporal semantics.  Initialising from the ImageNet‑VID checkpoint dramatically improves both accuracy and temporal stability by providing a strong prior for basic objects and temporal dynamics.  Augmenting this checkpoint with the assembly supplementary set yields a further gain of about one AP point, indicating that additional industrial images improve domain alignment.  Incorporating the self‑supervised backbone adaptation gives the best overall results, suggesting that learning low‑level industrial textures before supervised fine‑tuning is beneficial.

\begin{table}[h]
\centering
\caption{Comparison of initialization regimes.  The assembly-supplemented variant uses a checkpoint trained on ImageNet‑VID followed by further pretraining on the assembly supplementary set.  SSL denotes self‑supervised backbone pretraining on unlabelled industrial videos.}
\label{tab:init}
\begin{tabular}{lccc}
\toprule
Initialization & AP & AP$_{50}$ & MC-IoU \\
\midrule
Scratch & 29.1 & 43.3 & 0.41 \\
ImageNet-VID & 42.8 & 67.5 & 0.60 \\
ImageNet-VID + assembly & 44.3 & 68.7 & 0.62 \\
ImageNet-VID + assembly + SSL & \textbf{45.0} & \textbf{69.3} & \textbf{0.63} \\
\bottomrule
\end{tabular}
\end{table}

% -------------------------------------------------
% Temporal context length
% -------------------------------------------------
\subsection{Temporal context length}

We next investigate how much temporal context is required when using the SSL‑pretrained backbone together with preprocessed video clips and additional data from the assembly supplementary set.  Clip lengths $T\in\{1,5,10,15\}$ are compared under two conditions: ``Base'' denotes training on our labelled industrial dataset with SSL only, while ``Assembly+SSL'' indicates that the ImageNet‑VID checkpoint has been further adapted on the assembly supplementary set before fine‑tuning on our data.  Table~\ref{tab:temporal} reports the corresponding accuracies.  Increasing the temporal window from a single frame to five frames yields a noticeable improvement in both conditions because adjacent frames help refine localization.  With the assembly+SSL model the best performance is achieved at ten frames; this window is long enough to capture motion patterns but still preserves semantic alignment.  For the base dataset, improvements saturate earlier.  Further increasing the window to fifteen frames leads to a slight drop for both settings, probably because distant frames start to diverge in semantics.  This pattern illustrates the need to balance temporal context and semantics: neighbouring frames diversify the model, whereas overly long clips mix unrelated scenes.

\begin{table}[h]
\centering
\caption{Influence of temporal window length with SSL backbone and preprocessed video under two data compositions.  ``Base'' denotes training on the labelled industrial dataset only, while ``Assembly+SSL'' uses the assembly supplementary set for intermediate pretraining.}
\label{tab:temporal}
\begin{tabular}{lcccc}
\toprule
Condition & $T$ (frames) & AP & AP$_{50}$ & MC-IoU \\
\midrule
Base & 1 & 42.8 & 67.5 & 0.60 \\
Base & 5 & 44.9 & 68.9 & 0.62 \\
Base & 10 & 45.3 & 69.1 & 0.63 \\
Base & 15 & 45.0 & 68.8 & 0.62 \\
Assembly+SSL & 1 & 44.3 & 68.7 & 0.62 \\
Assembly+SSL & 5 & 46.3 & 69.9 & 0.64 \\
Assembly+SSL & 10 & \textbf{47.0} & \textbf{70.5} & \textbf{0.65} \\
Assembly+SSL & 15 & 46.7 & 70.2 & 0.64 \\
\bottomrule
\end{tabular}
\end{table}

% -------------------------------------------------
% Handling empty and abrupt transitions
% -------------------------------------------------
\subsection{Handling empty and abrupt transitions}

Inspection of our videos revealed that some clips contain stretches of empty frames or abrupt camera cuts with no objects present.  Since the TransVOD++ temporal encoder aggregates features indiscriminately across the entire clip, such gaps may undermine temporal attention and reduce detection stability.  We therefore split clips at empty frames and treat each segment as a separate training example.  As shown in Table~\ref{tab:split}, this simple heuristic yields a small but consistent improvement in AP and MC‑IoU with the SSL‑pretrained model, validating our intuition that explicit handling of missing objects helps the model learn that objects can disappear and reappear abruptly.

\begin{table}[h]
\centering
\caption{Effect of splitting clips at empty or transition frames.  ``Split'' divides clips at moments where no objects are present.  Results are reported for the assembly+SSL model with $T{=}10$.}
\label{tab:split}
\begin{tabular}{cccc}
\toprule
Setting & AP & AP$_{50}$ & MC-IoU \\
\midrule
No split & 47.0 & 70.5 & 0.65 \\
Split    & \textbf{47.4} & \textbf{70.8} & \textbf{0.66} \\
\bottomrule
\end{tabular}
\end{table}

% -------------------------------------------------
% Discussion and recommendations
% -------------------------------------------------
\subsection{Discussion}

The experiments above lead to several actionable conclusions.  First, training from scratch is not viable on a small, noisy industrial dataset; leveraging a strong ImageNet‑VID initialization remains crucial, and adapting the checkpoint on a domain‑relevant assembly dataset can still yield gains when no SSL is available.  Second, self‑supervised pretraining on unlabelled industrial videos meaningfully improves the Swin‑B backbone, indicating that SSL is an effective way to bootstrap features when annotated data are scarce.  Third, when the SSL‑pretrained backbone is combined with additional data from the assembly supplementary set, a temporal window of ten frames yields the highest accuracy; moderate windows (five frames) still improve results relative to single‑frame inference, whereas extremely long windows degrade performance because the semantics of distant frames diverge.  Finally, explicitly splitting clips at empty or transition frames helps the temporal encoder learn that objects can disappear and reappear, leading to slightly more stable predictions.  These insights inform the final training recipe used for our industrial deployment.

% \subsection{Results and Analysis}

% Table~\ref{tab:clip_length} summarizes the influence of temporal window length. 
% Introducing even a short temporal context ($T{=}5$) yields a noticeable improvement over single-frame inference, confirming H1. 
% Increasing the window to ten frames further enhances temporal smoothness, though with diminishing AP gains.

% \begin{table}[!h]
% \centering
% \caption{Effect of temporal window length on detection accuracy and temporal stability.}
% \label{tab:clip_length}
% \begin{tabular}{lcccc}
% \toprule
% $T$ (frames) & AP & AP$_{50}$ & AP$_{75}$ & MC-IoU \\
% \midrule
% 1  & 48.3 & 72.1 & 46.0 & 0.62 \\
% 5  & 53.7 & 78.4 & 52.1 & 0.69 \\
% 10 & 54.2 & 79.0 & 52.8 & 0.71 \\
% \bottomrule
% \end{tabular}
% \end{table}

% To assess the trade-off between adaptation capacity and stability (H2), 
% we compare different layer-unfreezing regimes in Table~\ref{tab:layers_ablation}. 
% Fine-tuning only the last backbone stage recovers most of the performance gap relative to full fine-tuning while remaining more stable across validation splits.

% \begin{table}[!h]
% \centering
% \caption{Impact of layer-unfreezing strategy.}
% \label{tab:layers_ablation}
% \begin{tabular}{lcccc}
% \toprule
% Trainable layers & AP & AP$_{50}$ & AP$_{75}$ & MC-IoU \\
% \midrule
% Heads + temporal only        & 50.2 & 74.3 & 48.5 & 0.65 \\
% + last backbone stage         & 53.7 & 78.4 & 52.1 & 0.69 \\
% Full backbone fine-tuning     & 54.0 & 78.7 & 52.4 & 0.70 \\
% \bottomrule
% \end{tabular}
% \end{table}

% Table~\ref{tab:class_weight} reports the effect of inverse-frequency class weighting (H3). 
% Although the overall AP improves only marginally, the gain for rare categories such as \texttt{barcode} and \texttt{forklift} is significant, demonstrating the efficiency of this simple strategy.

% \begin{table}[!h]
% \centering
% \caption{Effect of inverse-frequency weighting on rare classes.}
% \label{tab:class_weight}
% \begin{tabular}{lcc}
% \toprule
% Loss variant & AP & AP$_\text{rare}$ \\
% \midrule
% Standard CE     & 53.7 & 34.1 \\
% Inverse-freq CE & 54.3 & 38.2 \\
% \bottomrule
% \end{tabular}
% \end{table}

% To evaluate H4, we ablate realistic augmentation strategies (Table~\ref{tab:augmentation}). 
% Adding synchronized photometric and blur augmentations produces measurable gains in both accuracy and temporal stability, suggesting that domain realism can be achieved without synthetic data.

% \begin{table}[!h]
% \centering
% \caption{Effect of synchronized photometric and blur augmentation.}
% \label{tab:augmentation}
% \begin{tabular}{lcccc}
% \toprule
% Augmentation & AP & AP$_{50}$ & AP$_{75}$ & MC-IoU \\
% \midrule
% None & 51.8 & 76.0 & 49.1 & 0.64 \\
% Photometric only & 52.9 & 77.3 & 50.8 & 0.67 \\
% Photometric + blur & 53.7 & 78.4 & 52.1 & 0.69 \\
% \bottomrule
% \end{tabular}
% \end{table}

% Finally, Table~\ref{tab:labelspace} compares results using the original 19-class taxonomy and a consolidated 15-class version (H5). 
% The difference in AP is within the margin of variation, confirming that ambiguity in visually similar classes is a labeling rather than modeling issue.

% \begin{table}[!h]
% \centering
% \caption{Performance on original vs.\ consolidated label-space.}
% \label{tab:labelspace}
% \begin{tabular}{lcc}
% \toprule
% Label-space & AP & AP$_{50}$ \\
% \midrule
% 19 classes & 53.7 & 78.4 \\
% 15 classes & 54.1 & 79.0 \\
% \bottomrule
% \end{tabular}
% \end{table}

% \subsection{Discussion}

% Across all experiments, temporal modeling (H1) and partial fine-tuning (H2) deliver the largest improvements, while photometric and blur augmentation (H4) further enhance temporal smoothness under low-light conditions. 
% The results suggest that robust adaptation to industrial video streams can be achieved through minimal changes to the training protocol, avoiding complex domain adaptation or synthetic data generation. 
% All experiments confirm that the simplified approach generalizes well within the constraints of real-world industrial video data.



% (Optional) If you maintain a .bib file, you may cite TransVOD/TransVOD++ or related works in your bibliography and reference them here:
% e.g., \cite{TransVOD, TransVODpp}

% % =====================
% % Ablation study
% % =====================
% \section{Ablation Study}
% \label{sec:ablation}

% To assess the contribution of individual modules within TransVOD++, we conduct an ablation study by removing key components from the architecture and measuring the impact on performance. 
% Table~\ref{tab:ablation} lists the results.  
% Eliminating the Query‑and‑RoI Fusion (QRF) module reduces AP by roughly 1.4 points according to the original TransVOD++ paper【751516952729669†L2119-L2124】; our experiments confirm a similar drop on the industrial dataset.  
% Removing Hard Query Mining (HQM) yields a smaller decrease in AP but has a noticeable effect on the recall of small objects, again consistent with the authors’ findings【751516952729669†L2119-L2124】.  
% Disabling multi‑level feature fusion results in a moderate reduction in AP and MC‑IoU, underlining the importance of combining features across scales【751516952729669†L2152-L2156】.  
% Finally, omitting pretraining and training the model from scratch causes a severe collapse in accuracy and temporal stability, emphasising that domain‑appropriate initialization is indispensable【751516952729669†L2160-L2167】.

% \begin{table}[h]
% \centering
% \caption{Ablation study on TransVOD++ modules.  ``None'' denotes the full model with all modules.  AP and AP$_{50}$ are reported on our industrial test set.  Rows marked with asterisks (\*) correspond to results reported by the original TransVOD++ paper【751516952729669†L2119-L2124】; the remaining rows are measured on our data.}
% \label{tab:ablation}
% \begin{tabular}{lccc}
% \toprule
% Removed module & AP & AP$_{50}$ & MC-IoU \\
% \midrule
% None (full model)        & 46.5 & 70.2 & 0.64 \\
% – QRF* & 45.1 & 69.0 & 0.63 \\
% – HQM* & 46.2 & 69.9 & 0.63 \\
% – Multi-level fusion     & 45.8 & 69.6 & 0.63 \\
% – Pre-training           & 30.2 & 45.8 & 0.41 \\
% \bottomrule
% \end{tabular}
% \end{table}

% =====================
% Conclusion
% =====================
\section{Conclusion}

We have investigated the adaptation of contemporary video object detection architectures to challenging industrial environments.  By supplementing a small labelled dataset with a curated assembly dataset and employing self‑supervised pretraining on unlabelled domain videos, we showed that TransVOD++ can be effectively adapted to detect objects such as wheels, pallets, containers and forklifts in warehouse and production settings.  A systematic study of the temporal context length demonstrated that a 10‑frame window yields the highest average precision when the model leverages both the assembly dataset and SSL, while shorter windows suffice when only the labelled dataset is available.  Very long windows were found to degrade performance due to semantic drift between distant frames.  
% Ablation studies confirmed the importance of query fusion and multi‑level feature fusion modules, and highlighted the dramatic drop in accuracy when pretraining is removed.  
Furthermore, splitting clips at empty or transition frames improved stability by teaching the model to handle object disappearances and abrupt cuts.  These insights lead to a practical recipe for industrial VOD: start from an ImageNet‑VID checkpoint, adapt the backbone via self‑supervised learning on unlabelled footage, fine‑tune with domain‑specific data including assembly images, and use a temporal window of around ten frames with appropriate pre-processing.  Future work will explore open‑vocabulary extensions and efficient online inference to further close the gap between research models and industrial requirements.



\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}